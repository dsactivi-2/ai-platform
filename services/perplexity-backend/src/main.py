"""
Perplexity OSS - FastAPI application powered by OpenAI.

This application provides AI-powered search and chat functionality using specialized
OpenAI LLM agents for query processing, search, and response generation.
"""

import asyncio
import json
import os
import traceback
from typing import Generator

from dotenv import load_dotenv
from fastapi import FastAPI, Request, Depends
from fastapi.encoders import jsonable_encoder
from fastapi.middleware.cors import CORSMiddleware
from sse_starlette.sse import EventSourceResponse, ServerSentEvent

from agent_search import stream_pro_search_qa
from auth import get_authenticated_user, AuthenticatedUser
from chat import stream_qa_objects
from schemas import (
    ChatRequest,
    ChatResponseEvent,
    ErrorStream,
    StreamEvent,
)

load_dotenv()


def create_error_event(detail: str) -> ServerSentEvent:
    """Create a Server-Sent Event for error responses."""
    obj = ChatResponseEvent(
        data=ErrorStream(detail=detail),
        event=StreamEvent.ERROR,
    )
    return ServerSentEvent(
        data=json.dumps(jsonable_encoder(obj)),
        event=StreamEvent.ERROR,
    )


def create_app() -> FastAPI:
    """Create and configure the FastAPI application."""
    app = FastAPI(
        title="Perplexity OSS",
        description="AI-powered search engine powered by OpenAI",
        version="1.0.0",
    )

    # Configure CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    return app


app = create_app()

# Include OpenAI-compatible API routes
from api_compat.endpoints import router as compat_router
app.include_router(compat_router)


@app.on_event("startup")
async def startup_event():
    """Validate configuration on application startup."""
    import logging
    logger = logging.getLogger(__name__)

    print("\n" + "=" * 70)
    print("Perplexity OSS - Starting...")
    print("=" * 70 + "\n")

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.warning("OPENAI_API_KEY is not set - LLM features will not work.")
    else:
        print("OpenAI API key configured.")

    print("Startup complete.\n")


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "service": "perplexity-oss", "version": "2.0.0"}


@app.post("/chat")
async def chat(
    chat_request: ChatRequest, 
    request: Request,
    user: AuthenticatedUser = Depends(get_authenticated_user)
) -> EventSourceResponse:
    """
    Main chat endpoint that processes user queries using OpenAI.

    Supports both simple chat and advanced pro search modes.
    Returns a stream of responses including search results and AI-generated answers.
    Requires authentication.
    """
    async def generator():
        try:
            # Choose between simple chat and advanced pro search
            stream_fn = (
                stream_pro_search_qa if chat_request.pro_search else stream_qa_objects
            )
            
            async for obj in stream_fn(request=chat_request, session=None, user=user):
                if await request.is_disconnected():
                    break
                yield json.dumps(jsonable_encoder(obj))
                await asyncio.sleep(0)
                
        except Exception as e:
            print(f"Error in chat endpoint: {traceback.format_exc()}")
            # Ensure we always have a meaningful error message
            error_detail = str(e).strip() if str(e).strip() else "An unexpected error occurred during chat processing"
            error_type = type(e).__name__
            full_detail = f"{error_type}: {error_detail}" if error_detail != "An unexpected error occurred during chat processing" else error_detail
            yield create_error_event(full_detail)
            await asyncio.sleep(0)
            return

    return EventSourceResponse(generator(), media_type="text/event-stream")